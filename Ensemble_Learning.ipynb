{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "-->Ensemble Learning is a technique in machine learning where multiple models (called base learners or weak learners) are combined to make predictions.\n",
        "Key idea: Instead of relying on a single model, we combine the outputs of multiple models to reduce errors, improve accuracy, and increase robustness.\n",
        "\n",
        "---\n",
        "Q2. Difference between Bagging and Boosting\n",
        "-->Bagging (Bootstrap Aggregating) and Boosting are both ensemble learning techniques, but they work in different ways. Bagging trains multiple models in parallel using different bootstrap samples of the training data, and then combines their predictions (by voting in classification or averaging in regression) to reduce variance and prevent overfitting. Boosting, on the other hand, trains models sequentially, where each new model focuses on correcting the errors made by the previous ones. This process reduces bias and can improve accuracy, but it is more prone to overfitting if not properly regularized. While Bagging is mainly used to make unstable models like decision trees more stable, Boosting aims to build a strong learner by turning many weak learners into a powerful model.\n",
        "\n",
        "---\n",
        "Q3.Q3. Bootstrap sampling and its role in Bagging\n",
        "\n",
        "--->Bootstrap sampling is the process of creating multiple datasets by sampling with replacement from the original dataset.\n",
        "Role in Bagging: It ensures each model gets slightly different training data, making them diverse and reducing overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "Q4. Out-of-Bag (OOB) samples and OOB score\n",
        "--->OOB samples: Data points not included in a bootstrap sample for a given model.\n",
        "\n",
        "OOB score: Measures model accuracy by testing it on its OOB samples without using a separate validation se\n",
        "\n",
        "---\n",
        "Q5. Feature importance in Decision Tree vs Random Forest\n",
        "\n",
        "-->Decision Tree: Importance is based on how much each feature reduces impurity across all splits in the tree.\n",
        "\n",
        "Random Forest: Importance is averaged across all trees, giving a more reliable and less biased measure.\n",
        "\n",
        "---\n",
        "Q10. Step-by-step approach – Loan default prediction with ensemble learning\n",
        "\n",
        "-->Start with Boosting (e.g., XGBoost) if dataset is complex and has many features; Boosting handles bias better.\n",
        "\n",
        "Handle overfitting:\n",
        "\n",
        "Use cross-validation, tune learning rate, limit max depth.\n",
        "\n",
        "Select base models:\n",
        "\n",
        "Decision Trees for interpretability; Gradient Boosted Trees for performance.\n",
        "\n",
        "Evaluate performance:\n",
        "\n",
        "Use Stratified K-Fold cross-validation and metrics like ROC-AUC.\n",
        "\n",
        "Justification:\n",
        "\n",
        "Ensemble combines multiple weak learners to capture patterns better, improving loan default prediction accuracy."
      ],
      "metadata": {
        "id": "8O82PrIqpEpu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Q51SytjkpGzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q6. Python Program – Random Forest top 5 important features\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train Random Forest\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Get top 5 features\n",
        "importances = model.feature_importances_\n",
        "feature_names = data.feature_names\n",
        "top_indices = importances.argsort()[::-1][:5]\n",
        "\n",
        "for i in top_indices:\n",
        "    print(f\"{feature_names[i]}: {importances[i]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsEEZGRVp6UU",
        "outputId": "642582f5-d696-4529-c758-6a296a103122"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "worst area: 0.1394\n",
            "worst concave points: 0.1322\n",
            "mean concave points: 0.1070\n",
            "worst radius: 0.0828\n",
            "worst perimeter: 0.0808\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q7. Python Program – Bagging Classifier vs Decision Tree\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_acc = accuracy_score(y_test, dt.predict(X_test))\n",
        "\n",
        "# Bagging Classifier (updated parameter name)\n",
        "bagging = BaggingClassifier(estimator=DecisionTreeClassifier(),\n",
        "                            n_estimators=50, random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "bag_acc = accuracy_score(y_test, bagging.predict(X_test))\n",
        "\n",
        "print(f\"Decision Tree Accuracy: {dt_acc:.4f}\")\n",
        "print(f\"Bagging Accuracy: {bag_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C180HxUUqDbz",
        "outputId": "9aa71fee-6999-4ddc-97fe-260ba2bf75d2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0000\n",
            "Bagging Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8. Python Program – Random Forest with GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "\n",
        "# Load data\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Grid Search\n",
        "params = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 5, 10]\n",
        "}\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "grid = GridSearchCV(rf, params, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Accuracy:\", grid.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8XflxvvqSM6",
        "outputId": "3c055500-6bd0-45ff-b59c-f28f51c8310f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 100}\n",
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q9. Python Program – Bagging Regressor vs Random Forest Regressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Bagging Regressor\n",
        "bag_reg = BaggingRegressor(n_estimators=50, random_state=42)\n",
        "bag_reg.fit(X_train, y_train)\n",
        "bag_mse = mean_squared_error(y_test, bag_reg.predict(X_test))\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=50, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "rf_mse = mean_squared_error(y_test, rf_reg.predict(X_test))\n",
        "\n",
        "print(f\"Bagging MSE: {bag_mse:.4f}\")\n",
        "print(f\"Random Forest MSE: {rf_mse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kqLBTC3qbQS",
        "outputId": "58173e91-e31a-4363-abc3-aca487afdf1f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging MSE: 0.2573\n",
            "Random Forest MSE: 0.2573\n"
          ]
        }
      ]
    }
  ]
}